# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NHco7GjJXVRAHgk5MoBcM3JoFcghfxWx

METHOD 1- **TRAINING**
"""

student_id = 2202156

!pip install transformers

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

import numpy as np
import os
import pandas as pd
import string
import numpy
import io
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, f1_score, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from nltk.util import ngrams
import pickle
import nltk
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
stemmer = nltk.SnowballStemmer("english")
nltk.download('stopwords')
stopword = stopwords.words('english')

from sklearn.model_selection import train_test_split

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807/Assignment2/',str(student_id)) # Make sure to update with your student_id and student_id is an integer
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

# set same seeds for all libraries

#numpy seed
np.random.seed(student_id)

train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv') # This is 100% of data

print('Train 100% file: ', train_file)

valid_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')
print('Validation file: ', valid_file)

test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')
print('Test file: ', test_file)

test_df = pd.read_csv(test_file)
valid_df = pd.read_csv(valid_file)
train_df=pd.read_csv(train_file)

# Split the data into subsets of 25%, 50%, 75%, and 100% of the original dataset
subset_sizes = [0.25, 0.5, 0.75]

for size in subset_sizes:
    # Split the data into training and test sets using the current subset size
    train_data, test_data = train_test_split(train_df, train_size=size, test_size=1-size)
    
    # Print the size of the training and test sets
    print(f"Training set size: {len(train_data)}, Test set size: {len(test_data)}")
    
    # Train and evaluate your model on the current subset of the data
    # ...

train_df_25 = train_df.iloc[1:3078]
train_df_50 = train_df.iloc[1:6155]
train_df_75 = train_df.iloc[1:9234]
train_df_100 = train_df

MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1') 
print('Model 1 directory: ', MODEL_1_DIRECTORY)

MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') 
print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)

MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') 
print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)

MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75')
print('Model 1 directory with 175% data: ', MODEL_1_75_DIRECTORY)

MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50') 
print('Model 1 directory with 50% data: ', MODEL_1_50_DIRECTORY)

MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2') 
print('Model 2 directory: ', MODEL_2_DIRECTORY)

MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100') 
print('Model 2 directory with 100% data: ', MODEL_2_100_DIRECTORY)

MODEL_2_25_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25') 
print('Model 1 directory with 100% data: ', MODEL_2_25_DIRECTORY)

MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75') # Model 1 trained using 25% of train data directory
print('Model 1 directory with 100% data: ', MODEL_2_75_DIRECTORY)

MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50')
print('Model 1 directory with 100% data: ', MODEL_2_50_DIRECTORY)

def conv_dep(ctgry):
    if ctgry=='NOT':
        return 0
    elif ctgry=='OFF':
        return 1

def compute_performance(y_true, y_pred):
    print('Computing different preformance metrics')
    f1score=f1_score(y_true, y_pred, average='macro')
    acc = accuracy_score(y_true, y_pred)
    
    print('F1 Score(macro): ', f1score)
    print('Accuracy: ', acc)

def data_clnsing(dataset):
    dataset = re.sub('@', '', dataset)
    dataset = re.sub('\[.*?\]', '', dataset)
    dataset = re.sub('\n', '', dataset)
    dataset = re.sub('\w*\d\w*', '', dataset)
    dataset = str(dataset).lower()
    dataset = re.sub('https?://\S+|www\.\S+', '', dataset)
    dataset = re.sub('<.*?>+', '', dataset)
    dataset = re.sub('[%s]' % re.escape(string.punctuation), '', dataset)
    dataset = [stemmer.stem(word) for word in dataset.split(' ')]
    dataset=" ".join(dataset)
    dataset = [word for word in dataset.split(' ') if word not in stopword]
    dataset=" ".join(dataset)
    return dataset

def prepare_dataset1(data, tfidf_vectorizer=None, split=None):
  data['tweet']=data['tweet'].apply(data_clnsing)
  if split == 'train':
    tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_features=5000) 
    values = tfidf_vectorizer.fit_transform(data['tweet'].values) #TODO: This is the best way to do this, because you need to use same vectorization menthod
  else:
      values = tfidf_vectorizer.transform(data['tweet'].values)
  if split == 'train':
      return values, tfidf_vectorizer
  else:
      return values

def train_model1(text_vector,label):
  classifier = DecisionTreeClassifier()
  classifier=classifier.fit(text_vector,label)
  return classifier

def train_method1(train_file, valid_file, model_dir):
  
    train_values, tfidf_vectorizer = prepare_dataset1(train_file,tfidf_vectorizer=None,split="train")
    valid_values = prepare_dataset1(valid_file, tfidf_vectorizer=tfidf_vectorizer,split="valid")

    train_labels = train_file['label'].apply(conv_dep)
    valid_labels = valid_file['label'].apply(conv_dep)

    model = train_model1(train_values,train_labels)
    model_file, vectorizer_file = save_model1(model, tfidf_vectorizer, model_dir)


    train_pred_labels = model.predict(train_values)
    print('Train Split')
    compute_performance(train_labels, train_pred_labels)

    val_pred_label = model.predict(valid_values)
    print('Validation Split')
    compute_performance(valid_labels, val_pred_label)


    return model_file,vectorizer_file

def save_model1(model, vectorizer, model_dir):
    # save the model to disk
    model_file = os.path.join(model_dir, 'model.sav')
    pickle.dump(model, open(model_file, 'wb'))

    print('Saved model to ', model_file)

    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav') 
    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))

    print('Saved Vectorizer to ', vectorizer_file)

    return model_file, vectorizer_file

print('Train using of 100% of data')
model_100_file, vectorizer_100_file = train_method1(train_df_100, valid_df, MODEL_1_100_DIRECTORY)

print('Train using of 25% of data')
model_25_file, vectorizer_25_file = train_method1(train_df_25, valid_df, MODEL_1_25_DIRECTORY)

model_50_file, vectorizer_50_file = train_method1(train_df_50, valid_df, MODEL_1_50_DIRECTORY)

model_75_file, vectorizer_75_file = train_method1(train_df_75, valid_df, MODEL_1_75_DIRECTORY)

"""METHOD 1-**TESTING**"""

def load_model1(model_file, vectorizer_file):
    # load model and vectorizer from disk

    model = pickle.load(open(model_file, 'rb'))

    print('Loaded model from ', model_file)

    vectorizer = pickle.load(open(vectorizer_file, 'rb'))

    print('Loaded Vectorizer from ', vectorizer_file)


    return model, vectorizer

def test_method1(test_file, model_file, vectorizer_file, output_dir):
    test_label = test_df['label'].apply(conv_dep)
    model, vectorizer = load_model1(model_file, vectorizer_file) 
    test_values= prepare_dataset1(test_df,vectorizer)
    test_pred_label = model.predict(test_values)

    test_df['out_label']  = test_pred_label # Note how this is saved 
    test_f1_score = compute_performance(test_label, test_pred_label)
    out_file = os.path.join(output_dir, 'output_test.csv')
    print('Saving model output to', out_file)
    test_df.to_csv(out_file)

test_method1(test_df, model_100_file, vectorizer_100_file, MODEL_1_100_DIRECTORY)

test_method1(test_df, model_25_file, vectorizer_25_file, MODEL_1_25_DIRECTORY)

test_method1(test_df, model_50_file, vectorizer_50_file, MODEL_1_50_DIRECTORY)

test_method1(test_df, model_75_file, vectorizer_75_file, MODEL_1_75_DIRECTORY)

"""METHOD **2** """

def train_model2(text_vector,label):

    print('Let\'s start training Random Classifier')
    classifier = RandomForestClassifier(random_state=10)
    classifier.fit(text_vector, label)

    return classifier

train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv') # This is 100% of data

print('Train 100% file: ', train_file)

val_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')
print('Validation file: ', val_file)

test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')
print('Test file: ', test_file)

test_df = pd.read_csv(test_file)
valid_df = pd.read_csv(valid_file)
train_df=pd.read_csv(train_file)

train_df_25 = train_df.iloc[1:3078]
train_df_50 = train_df.iloc[1:6155]
train_df_75 = train_df.iloc[1:9234]
train_df_100 = train_df

def train_method2(train_file, valid_file, model_dir):
  
    train_values, tfidf_vectorizer = prepare_dataset1(train_file,tfidf_vectorizer=None,split="train")
    valid_values = prepare_dataset1(valid_file, tfidf_vectorizer=tfidf_vectorizer,split="valid")

    train_labels = train_file['label'].apply(conv_dep)
    valid_labels = valid_file['label'].apply(conv_dep)

    model = train_model2(train_values,train_labels)
    model_file, vectorizer_file = save_model1(model, tfidf_vectorizer, model_dir)


    train_pred_labels = model.predict(train_values)
    print('Train Split')
    compute_performance(train_labels, train_pred_labels)

    val_pred_label = model.predict(valid_values)
    print('Validation Split')
    compute_performance(valid_labels, val_pred_label)


    return model_file,vectorizer_file

print('Train using of 100% of data')
model_100_file, vectorizer_100_file = train_method2(train_df_100, valid_df, MODEL_2_100_DIRECTORY)

model_25_file, vectorizer_25_file = train_method2(train_df_25, valid_df, MODEL_2_25_DIRECTORY)

model_75_file, vectorizer_75_file = train_method2(train_df_75, valid_df, MODEL_2_75_DIRECTORY)

model_50_file, vectorizer_50_file = train_method2(train_df_50, valid_df, MODEL_2_50_DIRECTORY)

def test_method2(test_file, model_file, vectorizer_file, output_dir):
    test_label = test_df['label'].apply(conv_dep)
    model, vectorizer = load_model1(model_file, vectorizer_file) 
    test_values= prepare_dataset1(test_df,vectorizer)
    test_pred_label = model.predict(test_values)

    test_df['out_label']  = test_pred_label # Note how this is saved 
    test_f1_score = compute_performance(test_label, test_pred_label)
    out_file = os.path.join(output_dir, 'output_test.csv')
    print('Saving model output to', out_file)
    test_df.to_csv(out_file)

test_method2(test_df, model_100_file, vectorizer_100_file, MODEL_2_100_DIRECTORY)

test_method2(test_df, model_25_file, vectorizer_25_file, MODEL_2_25_DIRECTORY)

test_method2(test_df, model_50_file, vectorizer_50_file, MODEL_2_50_DIRECTORY)

test_method2(test_df, model_75_file, vectorizer_75_file, MODEL_2_75_DIRECTORY)

"""Comparing the test accuracies for all 5 splits"""

import matplotlib.pyplot as plt

M1 = [74.06,
73.37,
74.53,
75.34]
M2= [80.23,
81.74,
81.51,
82.20]

Size=[25, 50, 75, 100]

plt.plot(Size, M1)
plt.plot(Size, M2)
plt.xlabel('Different datasize')
plt.ylabel(' Test Accuracy')
plt.title("Accuracy plot between models for Test set")
plt.show()

import matplotlib.pyplot as plt

M1 = [0.69,
0.69,
0.71,
0.72]

M2= [0.74,
0.74,
0.75,
0.76

]

Size=[25, 50, 75, 100]

plt.plot(Size, M1)
plt.plot(Size, M2)
plt.xlabel('Different datasize')
plt.ylabel(' Validation Accuracy')
plt.title("Accuracy plot between models for valid set")
plt.show()

"""Testing with five samples for model 1 for 5 sets from the test dataset"""

a="@USER How is she hiding her ugly personality. She is the worst."
b="......bitch what URL"
c="#Conservatives @USER - You're  a clown!  URL"
d="@USER $500,000 for her wedding. Clinton's always know what's best."
e="#ArianaAsesina? Is that serious?! Holy shit, please your fucking assholes, don't blame someone for the death of other one. She is sad enough for today, don't you see? It isn't fault of none, he had an overdose and died. End. Stop wanting someone to blame, fuckers."

import joblib
#1 indicates OFF and 0 indicates not

MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') 
model_100_file = os.path.join(MODEL_1_100_DIRECTORY, "model.sav")
vectorizer_100_file = os.path.join(MODEL_1_100_DIRECTORY, "vectorizer.sav")

vectorizer = joblib.load(vectorizer_100_file)
model = joblib.load(model_100_file)

#A prediction-model 1-100
a_vectorized = vectorizer.transform([a])
prediction = model.predict(a_vectorized)
print(prediction,"Prediction for A")

#B prediction model 1-100

b_vectorized = vectorizer.transform([b])
prediction = model.predict(b_vectorized)
print(prediction,"Prediction for B")

#C prediction model 1-100

c_vectorized = vectorizer.transform([c])
prediction = model.predict(c_vectorized)
print(prediction,"Prediction for C")


#D prediction model 1-100

d_vectorized = vectorizer.transform([d])
prediction = model.predict(d_vectorized)
print(prediction,"Prediction for D")


#E prediction model 1-100

e_vectorized = vectorizer.transform([e])
prediction = model.predict(e_vectorized)
print(prediction,"Prediction for E")

#NO misclassification

MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') 
model_25_file = os.path.join(MODEL_1_25_DIRECTORY, "model.sav")
vectorizer_25_file = os.path.join(MODEL_1_25_DIRECTORY, "vectorizer.sav")

vectorizer = joblib.load(vectorizer_25_file)
model = joblib.load(model_25_file)

#A prediction-model 1-25
a_vectorized = vectorizer.transform([a])
prediction = model.predict(a_vectorized)
print(prediction,"Prediction for A")

#B prediction model 1-25

b_vectorized = vectorizer.transform([b])
prediction = model.predict(b_vectorized)
print(prediction,"Prediction for B")

#C prediction model 1-25

c_vectorized = vectorizer.transform([c])
prediction = model.predict(c_vectorized)
print(prediction,"Prediction for C")


#D prediction model 1-25

d_vectorized = vectorizer.transform([d])
prediction = model.predict(d_vectorized)
print(prediction,"Prediction for D")


#E prediction model 1-25

e_vectorized = vectorizer.transform([e])
prediction = model.predict(e_vectorized)
print(prediction,"Prediction for E")

#D has been misclassified

MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50') 
model_50_file = os.path.join(MODEL_1_50_DIRECTORY, "model.sav")
vectorizer_50_file = os.path.join(MODEL_1_50_DIRECTORY, "vectorizer.sav")

vectorizer = joblib.load(vectorizer_50_file)
model = joblib.load(model_50_file)

#A prediction-model 1-50
a_vectorized = vectorizer.transform([a])
prediction = model.predict(a_vectorized)
print(prediction,"Prediction for A")

#B prediction model 1-50

b_vectorized = vectorizer.transform([b])
prediction = model.predict(b_vectorized)
print(prediction,"Prediction for B")

#C prediction model 1-50

c_vectorized = vectorizer.transform([c])
prediction = model.predict(c_vectorized)
print(prediction,"Prediction for C")


#D prediction model 1-50

d_vectorized = vectorizer.transform([d])
prediction = model.predict(d_vectorized)
print(prediction,"Prediction for D")


#E prediction model 1-50

e_vectorized = vectorizer.transform([e])
prediction = model.predict(e_vectorized)
print(prediction,"Prediction for E")

#D has been misclassified

MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75') 
model_75_file = os.path.join(MODEL_1_75_DIRECTORY, "model.sav")
vectorizer_75_file = os.path.join(MODEL_1_75_DIRECTORY, "vectorizer.sav")

vectorizer = joblib.load(vectorizer_75_file)
model = joblib.load(model_75_file)

#A prediction-model 1-75
a_vectorized = vectorizer.transform([a])
prediction = model.predict(a_vectorized)
print(prediction,"Prediction for A")

#B prediction model 1-75

b_vectorized = vectorizer.transform([b])
prediction = model.predict(b_vectorized)
print(prediction,"Prediction for B")

#C prediction model 1-75

c_vectorized = vectorizer.transform([c])
prediction = model.predict(c_vectorized)
print(prediction,"Prediction for C")


#D prediction model 1-75

d_vectorized = vectorizer.transform([d])
prediction = model.predict(d_vectorized)
print(prediction,"Prediction for D")


#E prediction model 1-75

e_vectorized = vectorizer.transform([e])
prediction = model.predict(e_vectorized)
print(prediction,"Prediction for E")

#B and D misclassified

"""Testing with five samples for model 2 for 5 sets from the test dataset"""

MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100') 
model_100_file = os.path.join(MODEL_2_100_DIRECTORY, "model.sav")
vectorizer_100_file = os.path.join(MODEL_2_100_DIRECTORY, "vectorizer.sav")

vectorizer = joblib.load(vectorizer_100_file)
model = joblib.load(model_100_file)

#A prediction-model 2-100
a_vectorized = vectorizer.transform([a])
prediction = model.predict(a_vectorized)
print(prediction,"Prediction for A")

#B prediction model 2-100

b_vectorized = vectorizer.transform([b])
prediction = model.predict(b_vectorized)
print(prediction,"Prediction for B")

#C prediction model 2-100

c_vectorized = vectorizer.transform([c])
prediction = model.predict(c_vectorized)
print(prediction,"Prediction for C")


#D prediction model 2-100

d_vectorized = vectorizer.transform([d])
prediction = model.predict(d_vectorized)
print(prediction,"Prediction for D")


#E prediction model 2-100

e_vectorized = vectorizer.transform([e])
prediction = model.predict(e_vectorized)
print(prediction,"Prediction for E")

#no misclassification

MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25') 
model_25_file = os.path.join(MODEL_2_25_DIRECTORY, "model.sav")
vectorizer_25_file = os.path.join(MODEL_2_25_DIRECTORY, "vectorizer.sav")

vectorizer = joblib.load(vectorizer_25_file)
model = joblib.load(model_25_file)

#A prediction-model 2-25
a_vectorized = vectorizer.transform([a])
prediction = model.predict(a_vectorized)
print(prediction,"Prediction for A")

#B prediction model 2-25

b_vectorized = vectorizer.transform([b])
prediction = model.predict(b_vectorized)
print(prediction,"Prediction for B")

#C prediction model 2-25

c_vectorized = vectorizer.transform([c])
prediction = model.predict(c_vectorized)
print(prediction,"Prediction for C")


#D prediction model 2-25

d_vectorized = vectorizer.transform([d])
prediction = model.predict(d_vectorized)
print(prediction,"Prediction for D")


#E prediction model 2-25

e_vectorized = vectorizer.transform([e])
prediction = model.predict(e_vectorized)
print(prediction,"Prediction for E")

#no misclassification

MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50') 
model_50_file = os.path.join(MODEL_2_50_DIRECTORY, "model.sav")
vectorizer_50_file = os.path.join(MODEL_2_50_DIRECTORY, "vectorizer.sav")

vectorizer = joblib.load(vectorizer_50_file)
model = joblib.load(model_50_file)

#A prediction-model 2-50
a_vectorized = vectorizer.transform([a])
prediction = model.predict(a_vectorized)
print(prediction,"Prediction for A")

#B prediction model 2-50

b_vectorized = vectorizer.transform([b])
prediction = model.predict(b_vectorized)
print(prediction,"Prediction for B")

#C prediction model 2-50

c_vectorized = vectorizer.transform([c])
prediction = model.predict(c_vectorized)
print(prediction,"Prediction for C")


#D prediction model 2-50

d_vectorized = vectorizer.transform([d])
prediction = model.predict(d_vectorized)
print(prediction,"Prediction for D")


#E prediction model 2-50

e_vectorized = vectorizer.transform([e])
prediction = model.predict(e_vectorized)
print(prediction,"Prediction for E")

#no misclassification

MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75') 
model_75_file = os.path.join(MODEL_2_75_DIRECTORY, "model.sav")
vectorizer_75_file = os.path.join(MODEL_2_75_DIRECTORY, "vectorizer.sav")

vectorizer = joblib.load(vectorizer_75_file)
model = joblib.load(model_75_file)

#A prediction-model 2-75
a_vectorized = vectorizer.transform([a])
prediction = model.predict(a_vectorized)
print(prediction,"Prediction for A")

#B prediction model 2-75

b_vectorized = vectorizer.transform([b])
prediction = model.predict(b_vectorized)
print(prediction,"Prediction for B")

#C prediction model 2-75

c_vectorized = vectorizer.transform([c])
prediction = model.predict(c_vectorized)
print(prediction,"Prediction for C")


#D prediction model 2-75

d_vectorized = vectorizer.transform([d])
prediction = model.predict(d_vectorized)
print(prediction,"Prediction for D")


#E prediction model 2-75

e_vectorized = vectorizer.transform([e])
prediction = model.predict(e_vectorized)
print(prediction,"Prediction for E")

#no misclassification